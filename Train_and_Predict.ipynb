{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f813ba3",
   "metadata": {},
   "source": [
    "# SpotFake: Twitter Fake News Detection - Training & Inference\n",
    "\n",
    "This notebook provides a clean implementation for:\n",
    "1. Loading and preprocessing data (text + images)\n",
    "2. Building the multimodal model (BERT + VGG19)\n",
    "3. Training the model\n",
    "4. Making predictions on new inputs (text + image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813ea680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m venv .venv\n",
    "# !.venv\\Scripts\\Activate.ps1\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d5b7e4",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa47273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ML\\SpotFake02\\.venv\\Lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\ML\\SpotFake02\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "TensorFlow version: 2.20.0\n",
      "✓ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import gc\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Suppress TF warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a557368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "max_seq_length = 23\n",
    "img_length = 224\n",
    "img_width = 224\n",
    "img_channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63aebc6",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f5fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress callback\n",
    "def live():\n",
    "    \"\"\"Simple callback for training progress\"\"\"\n",
    "    return tf.keras.callbacks.LambdaCallback(\n",
    "        on_epoch_end=lambda epoch, logs: print(\n",
    "            f\"Epoch {epoch + 1}: loss={logs.get('loss', 0):.4f}, \"\n",
    "            f\"acc={logs.get('accuracy', 0):.4f}, \"\n",
    "            f\"val_loss={logs.get('val_loss', 0):.4f}, \"\n",
    "            f\"val_acc={logs.get('val_accuracy', 0):.4f}\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89817c75",
   "metadata": {},
   "source": [
    "### Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a29486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Text preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example for padding.\"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the BERT tokenizer.\"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    return tokenizer\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single InputExample into features.\"\"\"\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        example.text_a,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'][0].numpy().tolist()\n",
    "    input_mask = encoding['attention_mask'][0].numpy().tolist()\n",
    "    segment_ids = [0] * max_seq_length\n",
    "    \n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of InputExamples to features.\"\"\"\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples from texts and labels.\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=text if isinstance(text, str) else \" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples\n",
    "\n",
    "def preprocess_text_input(text, tokenizer, max_seq_length=23):\n",
    "    \"\"\"Preprocess a single text input for prediction.\"\"\"\n",
    "    example = InputExample(guid=None, text_a=text, text_b=None, label=0)\n",
    "    input_id, input_mask, segment_id, _ = convert_single_example(\n",
    "        tokenizer, example, max_seq_length\n",
    "    )\n",
    "    return np.array([input_id]), np.array([input_mask]), np.array([segment_id])\n",
    "\n",
    "print(\"✓ Text preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffd799",
   "metadata": {},
   "source": [
    "### Image Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0774ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Image preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "def read_and_process_image(list_of_images, length=224, width=224):\n",
    "    \"\"\"Read and preprocess multiple images.\"\"\"\n",
    "    X = [] \n",
    "    for image in tqdm(list_of_images, desc=\"Processing images\"):\n",
    "        X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (length, width), interpolation=cv2.INTER_CUBIC))  \n",
    "    return np.array(X)\n",
    "\n",
    "def preprocess_single_image(image_path, length=224, width=224):\n",
    "    \"\"\"Preprocess a single image for prediction.\"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image from {image_path}\")\n",
    "    img = cv2.resize(img, (length, width), interpolation=cv2.INTER_CUBIC)\n",
    "    # Convert to (channels, height, width) format\n",
    "    img = np.rollaxis(img, 2, 0)\n",
    "    return np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "print(\"✓ Image preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af4a44c",
   "metadata": {},
   "source": [
    "## 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e25c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model definition ready\n"
     ]
    }
   ],
   "source": [
    "def get_news_model(params):\n",
    "    \"\"\"Build the multimodal fake news detection model.\"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # BERT encoder function\n",
    "    def bert_encode(input_ids, input_mask, segment_ids):\n",
    "        bert_layer = hub.KerasLayer(\n",
    "            bert_path,\n",
    "            trainable=False,\n",
    "            signature=\"tokens\",\n",
    "            signature_outputs_as_dict=True,\n",
    "        )\n",
    "        bert_inputs = {\n",
    "            \"input_ids\": input_ids, \n",
    "            \"input_mask\": input_mask, \n",
    "            \"segment_ids\": segment_ids\n",
    "        }\n",
    "        bert_outputs = bert_layer(bert_inputs)\n",
    "        return bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    # Text input branch\n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\", dtype=tf.int32)\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\", dtype=tf.int32)\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\", dtype=tf.int32)\n",
    "    \n",
    "    bert_output = tf.keras.layers.Lambda(\n",
    "        lambda inputs: bert_encode(inputs[0], inputs[1], inputs[2]),\n",
    "        output_shape=(768,),\n",
    "        name=\"bert_encoding\"\n",
    "    )([in_id, in_mask, in_segment])\n",
    "\n",
    "    if params['text_no_hidden_layer'] > 0:\n",
    "        for i in range(params['text_no_hidden_layer']):\n",
    "            bert_output = tf.keras.layers.Dense(params['text_hidden_neurons'], activation='relu')(bert_output)\n",
    "            bert_output = tf.keras.layers.Dropout(params['dropout'])(bert_output)\n",
    "\n",
    "    text_repr = tf.keras.layers.Dense(params['repr_size'], activation='relu')(bert_output)\n",
    "\n",
    "    # Image input branch (VGG19)\n",
    "    conv_base = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    conv_base.trainable = False\n",
    "\n",
    "    input_image = tf.keras.layers.Input(shape=(3, 224, 224))\n",
    "    transposed_image = tf.keras.layers.Lambda(lambda x: tf.transpose(x, [0, 2, 3, 1]))(input_image)\n",
    "    base_output = conv_base(transposed_image)\n",
    "    flat = tf.keras.layers.Flatten()(base_output)\n",
    "\n",
    "    if params['vis_no_hidden_layer'] > 0:\n",
    "        for i in range(params['vis_no_hidden_layer']):\n",
    "            flat = tf.keras.layers.Dense(params['vis_hidden_neurons'], activation='relu')(flat)\n",
    "            flat = tf.keras.layers.Dropout(params['dropout'])(flat)\n",
    "\n",
    "    visual_repr = tf.keras.layers.Dense(params['repr_size'], activation='relu')(flat)\n",
    "\n",
    "    # Classifier (combine text + image)\n",
    "    combine_repr = tf.keras.layers.concatenate([text_repr, visual_repr])\n",
    "    com_drop = tf.keras.layers.Dropout(params['dropout'])(combine_repr)\n",
    "\n",
    "    if params['final_no_hidden_layer'] > 0:\n",
    "        for i in range(params['final_no_hidden_layer']):\n",
    "            com_drop = tf.keras.layers.Dense(params['final_hidden_neurons'], activation='relu')(com_drop)\n",
    "            com_drop = tf.keras.layers.Dropout(params['dropout'])(com_drop)\n",
    "\n",
    "    prediction = tf.keras.layers.Dense(1, activation='sigmoid')(com_drop)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[in_id, in_mask, in_segment, input_image], outputs=prediction)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=params['optimizer'](), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✓ Model definition ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2046241f",
   "metadata": {},
   "source": [
    "## 4. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1beab1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (15629, 7)\n",
      "Test shape: (2177, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>324597532548276224</td>\n",
       "      <td>Don't need feds to solve the #bostonbombing wh...</td>\n",
       "      <td>886672620</td>\n",
       "      <td>boston_fake_03,boston_fake_35</td>\n",
       "      <td>SantaCruzShred</td>\n",
       "      <td>Wed Apr 17 18:57:37 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>325145334739267584</td>\n",
       "      <td>PIC: Comparison of #Boston suspect Sunil Tripa...</td>\n",
       "      <td>21992286</td>\n",
       "      <td>boston_fake_23</td>\n",
       "      <td>Oscar_Wang</td>\n",
       "      <td>Fri Apr 19 07:14:23 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>325152091423248385</td>\n",
       "      <td>I'm not completely convinced that it's this Su...</td>\n",
       "      <td>16428755</td>\n",
       "      <td>boston_fake_34</td>\n",
       "      <td>jamwil</td>\n",
       "      <td>Fri Apr 19 07:41:14 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>324554646976868352</td>\n",
       "      <td>Brutal lo que se puede conseguir en colaboraci...</td>\n",
       "      <td>303138574</td>\n",
       "      <td>boston_fake_03,boston_fake_35</td>\n",
       "      <td>rubenson80</td>\n",
       "      <td>Wed Apr 17 16:07:12 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>324315545572896768</td>\n",
       "      <td>4chan and the bombing. just throwing it out th...</td>\n",
       "      <td>180460772</td>\n",
       "      <td>boston_fake_15</td>\n",
       "      <td>Slimlenny</td>\n",
       "      <td>Wed Apr 17 00:17:06 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              post_id                                          post_text  \\\n",
       "0  324597532548276224  Don't need feds to solve the #bostonbombing wh...   \n",
       "1  325145334739267584  PIC: Comparison of #Boston suspect Sunil Tripa...   \n",
       "2  325152091423248385  I'm not completely convinced that it's this Su...   \n",
       "3  324554646976868352  Brutal lo que se puede conseguir en colaboraci...   \n",
       "4  324315545572896768  4chan and the bombing. just throwing it out th...   \n",
       "\n",
       "     user_id                       image_id        username  \\\n",
       "0  886672620  boston_fake_03,boston_fake_35  SantaCruzShred   \n",
       "1   21992286                 boston_fake_23      Oscar_Wang   \n",
       "2   16428755                 boston_fake_34          jamwil   \n",
       "3  303138574  boston_fake_03,boston_fake_35      rubenson80   \n",
       "4  180460772                 boston_fake_15       Slimlenny   \n",
       "\n",
       "                        timestamp label  \n",
       "0  Wed Apr 17 18:57:37 +0000 2013  fake  \n",
       "1  Fri Apr 19 07:14:23 +0000 2013  fake  \n",
       "2  Fri Apr 19 07:41:14 +0000 2013  fake  \n",
       "3  Wed Apr 17 16:07:12 +0000 2013  fake  \n",
       "4  Wed Apr 17 00:17:06 +0000 2013  fake  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets\n",
    "def get_df(file):\n",
    "    return pd.read_csv(file, sep='\\t')\n",
    "\n",
    "train_df = get_df('dataset/twitter/train_posts.txt')\n",
    "test_df = get_df('dataset/twitter/test_posts.txt')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "769e9563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc0f86be527481e93b51ee2b2daae75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec4099aaf894f4f825a840f54d21117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract first image ID\n",
    "def return_first_image(row):\n",
    "    return row['image_id'].split(',')[0].strip()\n",
    "\n",
    "train_df['first_image_id'] = train_df.progress_apply(lambda row: return_first_image(row), axis=1)\n",
    "test_df['first_image_id'] = test_df.progress_apply(lambda row: return_first_image(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ffae17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering - Train: (13754, 8), Test: (1001, 8)\n"
     ]
    }
   ],
   "source": [
    "# Filter out missing images\n",
    "images_train_dataset = [i for i in train_df['first_image_id'].tolist()]\n",
    "images_train_folder = [i.split('.')[0].strip() for i in listdir('dataset/twitter/images_train')]\n",
    "images_train_not_available = set(images_train_dataset) - set(images_train_folder)\n",
    "images_train_not_available.add('boston_fake_10')\n",
    "\n",
    "images_test_dataset = [i.split(',')[0].strip() for i in test_df['image_id'].tolist()]\n",
    "images_test_folder = [i.split('.')[0].strip() for i in listdir('dataset/twitter/images_test/')]\n",
    "images_test_not_available = set(images_test_dataset) - set(images_test_folder)\n",
    "\n",
    "train_df = train_df[~train_df['first_image_id'].isin(images_train_not_available)]\n",
    "test_df = test_df[~test_df['first_image_id'].isin(images_test_not_available)]\n",
    "\n",
    "print(f\"After filtering - Train: {train_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35ff7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data counts: 13754 train, 1001 test\n"
     ]
    }
   ],
   "source": [
    "# Extract text and labels\n",
    "train_text = train_df['post_text'].tolist()\n",
    "test_text = test_df['post_text'].tolist()\n",
    "\n",
    "train_images = [i for i in train_df['first_image_id'].tolist()]\n",
    "test_images = [i for i in test_df['first_image_id'].tolist()]\n",
    "\n",
    "trainY = train_df['label'].tolist()\n",
    "trainY = [1 if i == 'real' else 0 for i in trainY]\n",
    "\n",
    "testY = test_df['label'].tolist()\n",
    "testY = [1 if i == 'real' else 0 for i in testY]\n",
    "\n",
    "print(f\"Data counts: {len(train_text)} train, {len(test_text)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4643edc",
   "metadata": {},
   "source": [
    "### Process Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51863cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebe35c5c02445238949af6d6463ead3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features:   0%|          | 0/13754 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131668345ac14853ac62b8efdcb5ec2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features:   0%|          | 0/1001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text features shape: (13754, 23)\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text, trainY)\n",
    "test_examples = convert_text_to_examples(test_text, testY)\n",
    "\n",
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, trainY_processed\n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "\n",
    "(test_input_ids, test_input_masks, test_segment_ids, testY_processed\n",
    ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)\n",
    "\n",
    "print(f\"Text features shape: {train_input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596aa2b",
   "metadata": {},
   "source": [
    "### Process Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2cea199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 1024 jpg, 2 png, 0 jpeg, 2 gif\n"
     ]
    }
   ],
   "source": [
    "# Get image file extensions\n",
    "images = listdir('dataset/twitter/images_train/')\n",
    "images.extend(listdir('dataset/twitter/images_test/'))\n",
    "jpg, png, jpeg, gif = [], [], [], []\n",
    "\n",
    "valid_extensions = {'jpg', 'png', 'jpeg', 'gif'}\n",
    "for i in images:\n",
    "    if '.' not in i or i.startswith('.'):\n",
    "        continue\n",
    "    name, ext = i.split('.')[0], i.split('.')[-1].lower()\n",
    "    if ext in valid_extensions:\n",
    "        if ext == 'jpg':\n",
    "            jpg.append(name)\n",
    "        elif ext == 'png':\n",
    "            png.append(name)\n",
    "        elif ext == 'jpeg':\n",
    "            jpeg.append(name)\n",
    "        elif ext == 'gif':\n",
    "            gif.append(name)\n",
    "\n",
    "def get_extension_of_file(file_name):\n",
    "    if file_name in jpg:\n",
    "        return '.jpg'\n",
    "    elif file_name in png:\n",
    "        return '.png'\n",
    "    elif file_name in jpeg:\n",
    "        return '.jpeg'\n",
    "    else:\n",
    "        return '.gif'\n",
    "\n",
    "print(f\"Found: {len(jpg)} jpg, {len(png)} png, {len(jpeg)} jpeg, {len(gif)} gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0345ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full image paths\n",
    "train_image_paths = ['dataset/twitter/images_train/' + i + get_extension_of_file(i) for i in train_images]\n",
    "test_image_paths = ['dataset/twitter/images_test/' + i + get_extension_of_file(i) for i in test_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69da7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded preprocessed images from .npy files\n",
      "Image data shape: (13754, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Load from saved .npy files (if available)\n",
    "try:\n",
    "    train_imagesX = np.load('train_imagesX.npy')\n",
    "    test_imagesX = np.load('test_imagesX.npy')\n",
    "    print(\"✓ Loaded preprocessed images from .npy files\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Processing images from scratch (this may take a while)...\")\n",
    "    train_imagesX = read_and_process_image(train_image_paths)\n",
    "    test_imagesX = read_and_process_image(test_image_paths)\n",
    "    \n",
    "    # Save for future use\n",
    "    np.save('train_imagesX.npy', train_imagesX)\n",
    "    np.save('test_imagesX.npy', test_imagesX)\n",
    "    print(\"✓ Saved preprocessed images to .npy files\")\n",
    "\n",
    "# Convert to (batch, channels, height, width) format\n",
    "train_imagesX = np.rollaxis(train_imagesX, 3, 1)\n",
    "test_imagesX = np.rollaxis(test_imagesX, 3, 1)\n",
    "\n",
    "print(f\"Image data shape: {train_imagesX.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a0fda",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "192243c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\n",
      "  text_no_hidden_layer: 1\n",
      "  text_hidden_neurons: 768\n",
      "  dropout: 0.4\n",
      "  repr_size: 32\n",
      "  vis_no_hidden_layer: 1\n",
      "  vis_hidden_neurons: 2742\n",
      "  final_no_hidden_layer: 1\n",
      "  final_hidden_neurons: 35\n",
      "  optimizer: <class 'keras.src.optimizers.adam.Adam'>\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters (from hyperparameter search)\n",
    "params_final = {\n",
    "    'text_no_hidden_layer': 1,\n",
    "    'text_hidden_neurons': 768,\n",
    "    'dropout': 0.4,\n",
    "    'repr_size': 32,\n",
    "    'vis_no_hidden_layer': 1,\n",
    "    'vis_hidden_neurons': 2742,\n",
    "    'final_no_hidden_layer': 1,\n",
    "    'final_hidden_neurons': 35,\n",
    "    'optimizer': tf.keras.optimizers.Adam\n",
    "}\n",
    "\n",
    "print(\"Model parameters:\")\n",
    "for k, v in params_final.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d28aed6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_masks         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ segment_ids         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vgg19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">20,024,384</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_encoding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │ input_masks[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ segment_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vgg19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span> │ bert_encoding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2742</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">68,794,038</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2742</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,608</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">87,776</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,275</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m224\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m224\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_masks         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ segment_ids         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vgg19 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m20,024,384\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_encoding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │ input_masks[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ segment_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ vgg19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │    \u001b[38;5;34m590,592\u001b[0m │ bert_encoding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2742\u001b[0m)      │ \u001b[38;5;34m68,794,038\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2742\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m24,608\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m87,776\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)        │      \u001b[38;5;34m2,275\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m36\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,523,709</span> (341.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m89,523,709\u001b[0m (341.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">69,499,325</span> (265.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m69,499,325\u001b[0m (265.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,024,384</span> (76.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,024,384\u001b[0m (76.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build model\n",
    "model = get_news_model(params_final)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a40de6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to: 0.0005000000237487257\n"
     ]
    }
   ],
   "source": [
    "# Set learning rate\n",
    "model.optimizer.learning_rate.assign(0.0005)\n",
    "print(f\"Learning rate set to: {model.optimizer.learning_rate.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2453f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint callback configured\n"
     ]
    }
   ],
   "source": [
    "# Setup checkpoint callback\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'model-{epoch:03d}-{val_accuracy:.6f}.h5', \n",
    "    verbose=1, \n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True, \n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "print(\"✓ Checkpoint callback configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f0db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "\u001b[1m  5/108\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30:35\u001b[0m 18s/step - accuracy: 0.5865 - loss: 5.1051"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids, train_imagesX], \n",
    "    trainY_processed,\n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    validation_data=(\n",
    "        [test_input_ids, test_input_masks, test_segment_ids, test_imagesX],\n",
    "        testY_processed\n",
    "    ),\n",
    "    callbacks=[live(), checkpoint]\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(history.history['loss'], label='Train Loss')\n",
    "ax2.plot(history.history['val_loss'], label='Val Loss')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4749746f",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model (update the filename with your best checkpoint)\n",
    "# Example: model.load_weights('model-010-0.776923.h5')\n",
    "\n",
    "# Or evaluate current model\n",
    "test_predictions = model.predict([\n",
    "    test_input_ids, test_input_masks, test_segment_ids, test_imagesX\n",
    "])\n",
    "test_predictions_binary = [1 if i >= 0.5 else 0 for i in test_predictions]\n",
    "\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(f\"Accuracy:  {accuracy_score(testY_processed, test_predictions_binary):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(testY_processed, test_predictions_binary, average=None)}\")\n",
    "print(f\"Precision: {precision_score(testY_processed, test_predictions_binary, average=None)}\")\n",
    "print(f\"Recall:    {recall_score(testY_processed, test_predictions_binary, average=None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8225a4",
   "metadata": {},
   "source": [
    "## 7. Inference on New Input\n",
    "\n",
    "Use this section to make predictions on new text + image pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fake_news(text, image_path, model, tokenizer, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict whether a news post (text + image) is fake or real.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The post text\n",
    "        image_path (str): Path to the image file\n",
    "        model: Trained Keras model\n",
    "        tokenizer: BERT tokenizer\n",
    "        threshold (float): Classification threshold (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    input_ids, input_masks, segment_ids = preprocess_text_input(\n",
    "        text, tokenizer, max_seq_length=max_seq_length\n",
    "    )\n",
    "    \n",
    "    # Preprocess image\n",
    "    image_data = preprocess_single_image(image_path, length=img_length, width=img_width)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(\n",
    "        [input_ids, input_masks, segment_ids, image_data],\n",
    "        verbose=0\n",
    "    )[0][0]\n",
    "    \n",
    "    # Classify\n",
    "    is_real = prediction >= threshold\n",
    "    label = \"REAL\" if is_real else \"FAKE\"\n",
    "    confidence = prediction if is_real else (1 - prediction)\n",
    "    \n",
    "    return {\n",
    "        'label': label,\n",
    "        'confidence': float(confidence),\n",
    "        'raw_score': float(prediction),\n",
    "        'text': text,\n",
    "        'image_path': image_path\n",
    "    }\n",
    "\n",
    "print(\"✓ Inference function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc687ab",
   "metadata": {},
   "source": [
    "### Example: Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Replace with your own text and image path\n",
    "\n",
    "sample_text = \"Breaking news: Major event happening now!\"\n",
    "sample_image_path = test_image_paths[0]  # Using first test image as example\n",
    "\n",
    "result = predict_fake_news(sample_text, sample_image_path, model, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTION RESULT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Text: {result['text']}\")\n",
    "print(f\"Image: {result['image_path']}\")\n",
    "print(f\"\\nPrediction: {result['label']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"Raw Score: {result['raw_score']:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Display the image\n",
    "img = cv2.imread(sample_image_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(f\"Prediction: {result['label']} ({result['confidence']:.2%} confidence)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0664c30",
   "metadata": {},
   "source": [
    "### Batch Prediction on Multiple Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(texts, image_paths, model, tokenizer, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make predictions on multiple text-image pairs.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings\n",
    "        image_paths (list): List of image file paths\n",
    "        model: Trained Keras model\n",
    "        tokenizer: BERT tokenizer\n",
    "        threshold (float): Classification threshold\n",
    "    \n",
    "    Returns:\n",
    "        list: List of prediction dictionaries\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for text, img_path in tqdm(zip(texts, image_paths), total=len(texts), desc=\"Making predictions\"):\n",
    "        try:\n",
    "            result = predict_fake_news(text, img_path, model, tokenizer, threshold)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            results.append({\n",
    "                'label': 'ERROR',\n",
    "                'confidence': 0.0,\n",
    "                'raw_score': 0.0,\n",
    "                'text': text,\n",
    "                'image_path': img_path,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    return results\n",
    "\n",
    "print(\"✓ Batch prediction function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict on first 5 test samples\n",
    "sample_size = 5\n",
    "sample_texts = test_text[:sample_size]\n",
    "sample_images = test_image_paths[:sample_size]\n",
    "sample_labels = testY[:sample_size]\n",
    "\n",
    "predictions = predict_batch(sample_texts, sample_images, model, tokenizer)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BATCH PREDICTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "for i, (pred, true_label) in enumerate(zip(predictions, sample_labels)):\n",
    "    true_label_str = \"REAL\" if true_label == 1 else \"FAKE\"\n",
    "    match = \"✓\" if pred['label'] == true_label_str else \"✗\"\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Text: {pred['text'][:50]}...\")\n",
    "    print(f\"  True Label: {true_label_str}\")\n",
    "    print(f\"  Predicted: {pred['label']} ({pred['confidence']:.2%}) {match}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623e65b",
   "metadata": {},
   "source": [
    "## 8. Save/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c17fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "model.save('spotfake_final_model.h5')\n",
    "print(\"✓ Model saved to 'spotfake_final_model.h5'\")\n",
    "\n",
    "# To load the model later:\n",
    "# loaded_model = tf.keras.models.load_model('spotfake_final_model.h5', custom_objects={'KerasLayer': hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d24e37",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. **Data Loading & Preprocessing**: Load Twitter fake news dataset with text and images\n",
    "2. **Model Architecture**: Multimodal model combining BERT (text) + VGG19 (image)\n",
    "3. **Training**: Train the model with optimal hyperparameters\n",
    "4. **Evaluation**: Comprehensive metrics on test set\n",
    "5. **Inference**: Easy-to-use functions for predicting on new inputs\n",
    "\n",
    "### To use this notebook:\n",
    "\n",
    "1. Run all cells in order to train the model\n",
    "2. Use `predict_fake_news()` for single predictions\n",
    "3. Use `predict_batch()` for multiple predictions\n",
    "4. Load saved weights with `model.load_weights()` to skip training\n",
    "\n",
    "### For new predictions:\n",
    "\n",
    "```python\n",
    "# Your custom input\n",
    "my_text = \"Your news text here\"\n",
    "my_image = \"path/to/your/image.jpg\"\n",
    "\n",
    "# Get prediction\n",
    "result = predict_fake_news(my_text, my_image, model, tokenizer)\n",
    "print(f\"Prediction: {result['label']} (Confidence: {result['confidence']:.2%})\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
