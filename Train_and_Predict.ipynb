{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f813ba3",
   "metadata": {},
   "source": [
    "# SpotFake: Twitter Fake News Detection - Training & Inference\n",
    "\n",
    "This notebook provides a clean implementation for:\n",
    "1. Loading and preprocessing data (text + images)\n",
    "2. Building the multimodal model (BERT + VGG19)\n",
    "3. Training the model\n",
    "4. Making predictions on new inputs (text + image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "813ea680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m venv .venv\n",
    "# !.venv\\Scripts\\Activate.ps1\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d5b7e4",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa47273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ML\\SpotFake02\\.venv\\Lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\ML\\SpotFake02\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "TensorFlow version: 2.20.0\n",
      "✓ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import gc\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Suppress TF warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a557368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "max_seq_length = 23\n",
    "img_length = 224\n",
    "img_width = 224\n",
    "img_channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63aebc6",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f5fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress callback\n",
    "def live():\n",
    "    \"\"\"Simple callback for training progress\"\"\"\n",
    "    return tf.keras.callbacks.LambdaCallback(\n",
    "        on_epoch_end=lambda epoch, logs: print(\n",
    "            f\"Epoch {epoch + 1}: loss={logs.get('loss', 0):.4f}, \"\n",
    "            f\"acc={logs.get('accuracy', 0):.4f}, \"\n",
    "            f\"val_loss={logs.get('val_loss', 0):.4f}, \"\n",
    "            f\"val_acc={logs.get('val_accuracy', 0):.4f}\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89817c75",
   "metadata": {},
   "source": [
    "### Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a29486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Text preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example for padding.\"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the BERT tokenizer.\"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    return tokenizer\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single InputExample into features.\"\"\"\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        example.text_a,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'][0].numpy().tolist()\n",
    "    input_mask = encoding['attention_mask'][0].numpy().tolist()\n",
    "    segment_ids = [0] * max_seq_length\n",
    "    \n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of InputExamples to features.\"\"\"\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples from texts and labels.\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=text if isinstance(text, str) else \" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples\n",
    "\n",
    "def preprocess_text_input(text, tokenizer, max_seq_length=23):\n",
    "    \"\"\"Preprocess a single text input for prediction.\"\"\"\n",
    "    example = InputExample(guid=None, text_a=text, text_b=None, label=0)\n",
    "    input_id, input_mask, segment_id, _ = convert_single_example(\n",
    "        tokenizer, example, max_seq_length\n",
    "    )\n",
    "    return np.array([input_id]), np.array([input_mask]), np.array([segment_id])\n",
    "\n",
    "print(\"✓ Text preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffd799",
   "metadata": {},
   "source": [
    "### Image Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0774ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Image preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "def read_and_process_image(list_of_images, length=224, width=224):\n",
    "    \"\"\"Read and preprocess multiple images.\"\"\"\n",
    "    X = [] \n",
    "    for image in tqdm(list_of_images, desc=\"Processing images\"):\n",
    "        X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (length, width), interpolation=cv2.INTER_CUBIC))  \n",
    "    return np.array(X)\n",
    "\n",
    "def preprocess_single_image(image_path, length=224, width=224):\n",
    "    \"\"\"Preprocess a single image for prediction.\"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image from {image_path}\")\n",
    "    img = cv2.resize(img, (length, width), interpolation=cv2.INTER_CUBIC)\n",
    "    # Convert to (channels, height, width) format\n",
    "    img = np.rollaxis(img, 2, 0)\n",
    "    return np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "print(\"✓ Image preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af4a44c",
   "metadata": {},
   "source": [
    "## 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e25c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model definition ready\n"
     ]
    }
   ],
   "source": [
    "def get_news_model(params):\n",
    "    \"\"\"Build the multimodal fake news detection model.\"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # BERT encoder function\n",
    "    def bert_encode(input_ids, input_mask, segment_ids):\n",
    "        bert_layer = hub.KerasLayer(\n",
    "            bert_path,\n",
    "            trainable=False,\n",
    "            signature=\"tokens\",\n",
    "            signature_outputs_as_dict=True,\n",
    "        )\n",
    "        bert_inputs = {\n",
    "            \"input_ids\": input_ids, \n",
    "            \"input_mask\": input_mask, \n",
    "            \"segment_ids\": segment_ids\n",
    "        }\n",
    "        bert_outputs = bert_layer(bert_inputs)\n",
    "        return bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    # Text input branch\n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\", dtype=tf.int32)\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\", dtype=tf.int32)\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\", dtype=tf.int32)\n",
    "    \n",
    "    bert_output = tf.keras.layers.Lambda(\n",
    "        lambda inputs: bert_encode(inputs[0], inputs[1], inputs[2]),\n",
    "        output_shape=(768,),\n",
    "        name=\"bert_encoding\"\n",
    "    )([in_id, in_mask, in_segment])\n",
    "\n",
    "    if params['text_no_hidden_layer'] > 0:\n",
    "        for i in range(params['text_no_hidden_layer']):\n",
    "            bert_output = tf.keras.layers.Dense(params['text_hidden_neurons'], activation='relu')(bert_output)\n",
    "            bert_output = tf.keras.layers.Dropout(params['dropout'])(bert_output)\n",
    "\n",
    "    text_repr = tf.keras.layers.Dense(params['repr_size'], activation='relu')(bert_output)\n",
    "\n",
    "    # Image input branch (VGG19)\n",
    "    conv_base = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    conv_base.trainable = False\n",
    "\n",
    "    input_image = tf.keras.layers.Input(shape=(3, 224, 224))\n",
    "    transposed_image = tf.keras.layers.Lambda(lambda x: tf.transpose(x, [0, 2, 3, 1]))(input_image)\n",
    "    base_output = conv_base(transposed_image)\n",
    "    flat = tf.keras.layers.Flatten()(base_output)\n",
    "\n",
    "    if params['vis_no_hidden_layer'] > 0:\n",
    "        for i in range(params['vis_no_hidden_layer']):\n",
    "            flat = tf.keras.layers.Dense(params['vis_hidden_neurons'], activation='relu')(flat)\n",
    "            flat = tf.keras.layers.Dropout(params['dropout'])(flat)\n",
    "\n",
    "    visual_repr = tf.keras.layers.Dense(params['repr_size'], activation='relu')(flat)\n",
    "\n",
    "    # Classifier (combine text + image)\n",
    "    combine_repr = tf.keras.layers.concatenate([text_repr, visual_repr])\n",
    "    com_drop = tf.keras.layers.Dropout(params['dropout'])(combine_repr)\n",
    "\n",
    "    if params['final_no_hidden_layer'] > 0:\n",
    "        for i in range(params['final_no_hidden_layer']):\n",
    "            com_drop = tf.keras.layers.Dense(params['final_hidden_neurons'], activation='relu')(com_drop)\n",
    "            com_drop = tf.keras.layers.Dropout(params['dropout'])(com_drop)\n",
    "\n",
    "    prediction = tf.keras.layers.Dense(1, activation='sigmoid')(com_drop)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[in_id, in_mask, in_segment, input_image], outputs=prediction)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=params['optimizer'](), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✓ Model definition ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2046241f",
   "metadata": {},
   "source": [
    "## 4. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1beab1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (15629, 7)\n",
      "Test shape: (2177, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>324597532548276224</td>\n",
       "      <td>Don't need feds to solve the #bostonbombing wh...</td>\n",
       "      <td>886672620</td>\n",
       "      <td>boston_fake_03,boston_fake_35</td>\n",
       "      <td>SantaCruzShred</td>\n",
       "      <td>Wed Apr 17 18:57:37 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>325145334739267584</td>\n",
       "      <td>PIC: Comparison of #Boston suspect Sunil Tripa...</td>\n",
       "      <td>21992286</td>\n",
       "      <td>boston_fake_23</td>\n",
       "      <td>Oscar_Wang</td>\n",
       "      <td>Fri Apr 19 07:14:23 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>325152091423248385</td>\n",
       "      <td>I'm not completely convinced that it's this Su...</td>\n",
       "      <td>16428755</td>\n",
       "      <td>boston_fake_34</td>\n",
       "      <td>jamwil</td>\n",
       "      <td>Fri Apr 19 07:41:14 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>324554646976868352</td>\n",
       "      <td>Brutal lo que se puede conseguir en colaboraci...</td>\n",
       "      <td>303138574</td>\n",
       "      <td>boston_fake_03,boston_fake_35</td>\n",
       "      <td>rubenson80</td>\n",
       "      <td>Wed Apr 17 16:07:12 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>324315545572896768</td>\n",
       "      <td>4chan and the bombing. just throwing it out th...</td>\n",
       "      <td>180460772</td>\n",
       "      <td>boston_fake_15</td>\n",
       "      <td>Slimlenny</td>\n",
       "      <td>Wed Apr 17 00:17:06 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              post_id                                          post_text  \\\n",
       "0  324597532548276224  Don't need feds to solve the #bostonbombing wh...   \n",
       "1  325145334739267584  PIC: Comparison of #Boston suspect Sunil Tripa...   \n",
       "2  325152091423248385  I'm not completely convinced that it's this Su...   \n",
       "3  324554646976868352  Brutal lo que se puede conseguir en colaboraci...   \n",
       "4  324315545572896768  4chan and the bombing. just throwing it out th...   \n",
       "\n",
       "     user_id                       image_id        username  \\\n",
       "0  886672620  boston_fake_03,boston_fake_35  SantaCruzShred   \n",
       "1   21992286                 boston_fake_23      Oscar_Wang   \n",
       "2   16428755                 boston_fake_34          jamwil   \n",
       "3  303138574  boston_fake_03,boston_fake_35      rubenson80   \n",
       "4  180460772                 boston_fake_15       Slimlenny   \n",
       "\n",
       "                        timestamp label  \n",
       "0  Wed Apr 17 18:57:37 +0000 2013  fake  \n",
       "1  Fri Apr 19 07:14:23 +0000 2013  fake  \n",
       "2  Fri Apr 19 07:41:14 +0000 2013  fake  \n",
       "3  Wed Apr 17 16:07:12 +0000 2013  fake  \n",
       "4  Wed Apr 17 00:17:06 +0000 2013  fake  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets\n",
    "def get_df(file):\n",
    "    return pd.read_csv(file, sep='\\t')\n",
    "\n",
    "train_df = get_df('dataset/twitter/train_posts.txt')\n",
    "test_df = get_df('dataset/twitter/test_posts.txt')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769e9563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa88d7277e3846cd8dd74eb06e012305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02cba8114ba4efc8b8a025af78037ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract first image ID\n",
    "def return_first_image(row):\n",
    "    return row['image_id'].split(',')[0].strip()\n",
    "\n",
    "train_df['first_image_id'] = train_df.progress_apply(lambda row: return_first_image(row), axis=1)\n",
    "test_df['first_image_id'] = test_df.progress_apply(lambda row: return_first_image(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ffae17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering - Train: (13754, 8), Test: (1001, 8)\n"
     ]
    }
   ],
   "source": [
    "# Filter out missing images\n",
    "images_train_dataset = [i for i in train_df['first_image_id'].tolist()]\n",
    "images_train_folder = [i.split('.')[0].strip() for i in listdir('dataset/twitter/images_train')]\n",
    "images_train_not_available = set(images_train_dataset) - set(images_train_folder)\n",
    "images_train_not_available.add('boston_fake_10')\n",
    "\n",
    "images_test_dataset = [i.split(',')[0].strip() for i in test_df['image_id'].tolist()]\n",
    "images_test_folder = [i.split('.')[0].strip() for i in listdir('dataset/twitter/images_test/')]\n",
    "images_test_not_available = set(images_test_dataset) - set(images_test_folder)\n",
    "\n",
    "train_df = train_df[~train_df['first_image_id'].isin(images_train_not_available)]\n",
    "test_df = test_df[~test_df['first_image_id'].isin(images_test_not_available)]\n",
    "\n",
    "print(f\"After filtering - Train: {train_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a35ff7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data counts: 13754 train, 1001 test\n"
     ]
    }
   ],
   "source": [
    "# Extract text and labels\n",
    "train_text = train_df['post_text'].tolist()\n",
    "test_text = test_df['post_text'].tolist()\n",
    "\n",
    "train_images = [i for i in train_df['first_image_id'].tolist()]\n",
    "test_images = [i for i in test_df['first_image_id'].tolist()]\n",
    "\n",
    "trainY = train_df['label'].tolist()\n",
    "trainY = [1 if i == 'real' else 0 for i in trainY]\n",
    "\n",
    "testY = test_df['label'].tolist()\n",
    "testY = [1 if i == 'real' else 0 for i in testY]\n",
    "\n",
    "print(f\"Data counts: {len(train_text)} train, {len(test_text)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4643edc",
   "metadata": {},
   "source": [
    "### Process Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51863cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025a372de41147fba4a4ae19cd5b8ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features:   0%|          | 0/13754 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18cbbe89c1af4a458c3d8bff6d93ed8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting examples to features:   0%|          | 0/1001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text features shape: (13754, 23)\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text, trainY)\n",
    "test_examples = convert_text_to_examples(test_text, testY)\n",
    "\n",
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, trainY_processed\n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "\n",
    "(test_input_ids, test_input_masks, test_segment_ids, testY_processed\n",
    ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)\n",
    "\n",
    "print(f\"Text features shape: {train_input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596aa2b",
   "metadata": {},
   "source": [
    "### Process Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2cea199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 1024 jpg, 2 png, 0 jpeg, 2 gif\n"
     ]
    }
   ],
   "source": [
    "# Get image file extensions\n",
    "images = listdir('dataset/twitter/images_train/')\n",
    "images.extend(listdir('dataset/twitter/images_test/'))\n",
    "jpg, png, jpeg, gif = [], [], [], []\n",
    "\n",
    "valid_extensions = {'jpg', 'png', 'jpeg', 'gif'}\n",
    "for i in images:\n",
    "    if '.' not in i or i.startswith('.'):\n",
    "        continue\n",
    "    name, ext = i.split('.')[0], i.split('.')[-1].lower()\n",
    "    if ext in valid_extensions:\n",
    "        if ext == 'jpg':\n",
    "            jpg.append(name)\n",
    "        elif ext == 'png':\n",
    "            png.append(name)\n",
    "        elif ext == 'jpeg':\n",
    "            jpeg.append(name)\n",
    "        elif ext == 'gif':\n",
    "            gif.append(name)\n",
    "\n",
    "def get_extension_of_file(file_name):\n",
    "    if file_name in jpg:\n",
    "        return '.jpg'\n",
    "    elif file_name in png:\n",
    "        return '.png'\n",
    "    elif file_name in jpeg:\n",
    "        return '.jpeg'\n",
    "    else:\n",
    "        return '.gif'\n",
    "\n",
    "print(f\"Found: {len(jpg)} jpg, {len(png)} png, {len(jpeg)} jpeg, {len(gif)} gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0345ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full image paths\n",
    "train_image_paths = ['dataset/twitter/images_train/' + i + get_extension_of_file(i) for i in train_images]\n",
    "test_image_paths = ['dataset/twitter/images_test/' + i + get_extension_of_file(i) for i in test_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69da7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded preprocessed images from .npy files\n",
      "Image data shape: (13754, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Load from saved .npy files (if available)\n",
    "try:\n",
    "    train_imagesX = np.load('train_imagesX.npy')\n",
    "    test_imagesX = np.load('test_imagesX.npy')\n",
    "    print(\"✓ Loaded preprocessed images from .npy files\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Processing images from scratch (this may take a while)...\")\n",
    "    train_imagesX = read_and_process_image(train_image_paths)\n",
    "    test_imagesX = read_and_process_image(test_image_paths)\n",
    "    \n",
    "    # Save for future use\n",
    "    np.save('train_imagesX.npy', train_imagesX)\n",
    "    np.save('test_imagesX.npy', test_imagesX)\n",
    "    print(\"✓ Saved preprocessed images to .npy files\")\n",
    "\n",
    "# Convert to (batch, channels, height, width) format\n",
    "train_imagesX = np.rollaxis(train_imagesX, 3, 1)\n",
    "test_imagesX = np.rollaxis(test_imagesX, 3, 1)\n",
    "\n",
    "print(f\"Image data shape: {train_imagesX.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a0fda",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "192243c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\n",
      "  text_no_hidden_layer: 1\n",
      "  text_hidden_neurons: 768\n",
      "  dropout: 0.4\n",
      "  repr_size: 32\n",
      "  vis_no_hidden_layer: 1\n",
      "  vis_hidden_neurons: 2742\n",
      "  final_no_hidden_layer: 1\n",
      "  final_hidden_neurons: 35\n",
      "  optimizer: <class 'keras.src.optimizers.adam.Adam'>\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters (from hyperparameter search)\n",
    "params_final = {\n",
    "    'text_no_hidden_layer': 1,\n",
    "    'text_hidden_neurons': 768,\n",
    "    'dropout': 0.4,\n",
    "    'repr_size': 32,\n",
    "    'vis_no_hidden_layer': 1,\n",
    "    'vis_hidden_neurons': 2742,\n",
    "    'final_no_hidden_layer': 1,\n",
    "    'final_hidden_neurons': 35,\n",
    "    'optimizer': tf.keras.optimizers.Adam\n",
    "}\n",
    "\n",
    "print(\"Model parameters:\")\n",
    "for k, v in params_final.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d28aed6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_masks         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ segment_ids         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vgg19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">20,024,384</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_encoding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │ input_masks[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ segment_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vgg19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span> │ bert_encoding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2742</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">68,794,038</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2742</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,608</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">87,776</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,275</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m224\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m224\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_masks         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ segment_ids         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vgg19 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m20,024,384\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_encoding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │ input_masks[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ segment_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ vgg19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │    \u001b[38;5;34m590,592\u001b[0m │ bert_encoding[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2742\u001b[0m)      │ \u001b[38;5;34m68,794,038\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2742\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m24,608\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m87,776\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)        │      \u001b[38;5;34m2,275\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m36\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,523,709</span> (341.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m89,523,709\u001b[0m (341.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">69,499,325</span> (265.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m69,499,325\u001b[0m (265.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,024,384</span> (76.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,024,384\u001b[0m (76.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build model\n",
    "model = get_news_model(params_final)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a40de6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to: 0.0005000000237487257\n"
     ]
    }
   ],
   "source": [
    "# Set learning rate\n",
    "model.optimizer.learning_rate.assign(0.0005)\n",
    "print(f\"Learning rate set to: {model.optimizer.learning_rate.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2453f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint callback configured\n"
     ]
    }
   ],
   "source": [
    "# Setup checkpoint callback - DISABLED due to disk space\n",
    "# Note: Checkpoint is commented out to avoid \"No space left on device\" errors\n",
    "# You can save the model manually after training using model.save_weights()\n",
    "\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     'model_weights_{epoch:03d}_{val_accuracy:.4f}.weights.h5', \n",
    "#     verbose=1, \n",
    "#     monitor='val_accuracy',\n",
    "#     save_best_only=True,\n",
    "#     save_weights_only=True,\n",
    "#     mode='max'\n",
    "# )\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,  # Keeps best weights in memory\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"✓ Training callbacks configured (checkpoint disabled to save disk space)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c5f0db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36s/step - accuracy: 0.7621 - loss: 0.5278 \n",
      "Epoch 1: val_accuracy improved from None to 0.50250, saving model to model_weights_001_0.5025.weights.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.50250, saving model to model_weights_001_0.5025.weights.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] Can't synchronously write data (file write failed: time = Wed Oct 15 04:08:26 2025\n, filename = 'model_weights_001_0.5025.weights.h5', file descriptor = 4, errno = 28, error message = 'No space left on device', buf = 0000017778E61040, total write size = 9437184, bytes this sub-write = 9437184, offset = 61289672)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_input_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_segment_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_imagesX\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainY_processed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased batch size for efficiency\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Full training epochs\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_input_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_segment_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_imagesX\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtestY_processed\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Using checkpoint and early stopping\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Training completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\SpotFake02\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\SpotFake02\\.venv\\Lib\\site-packages\\h5py\\_hl\\group.py:486\u001b[39m, in \u001b[36mGroup.__setitem__\u001b[39m\u001b[34m(self, name, obj)\u001b[39m\n\u001b[32m    483\u001b[39m     htype.commit(\u001b[38;5;28mself\u001b[39m.id, name, lcpl=lcpl)\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m     ds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m     h5o.link(ds.id, \u001b[38;5;28mself\u001b[39m.id, name, lcpl=lcpl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\SpotFake02\\.venv\\Lib\\site-packages\\h5py\\_hl\\group.py:186\u001b[39m, in \u001b[36mGroup.create_dataset\u001b[39m\u001b[34m(self, name, shape, dtype, data, **kwds)\u001b[39m\n\u001b[32m    183\u001b[39m         parent_path, name = name.rsplit(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    184\u001b[39m         group = \u001b[38;5;28mself\u001b[39m.require_group(parent_path)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m dsid = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m dset = dataset.Dataset(dsid)\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML\\SpotFake02\\.venv\\Lib\\site-packages\\h5py\\_hl\\dataset.py:178\u001b[39m, in \u001b[36mmake_new_dset\u001b[39m\u001b[34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0, fill_time)\u001b[39m\n\u001b[32m    175\u001b[39m dset_id = h5d.create(parent.id, name, tid, sid, dcpl=dcpl, dapl=dapl)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[43mdset_id\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5s\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5s\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dset_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5d.pyx:307\u001b[39m, in \u001b[36mh5py.h5d.DatasetID.write\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_proxy.pyx:121\u001b[39m, in \u001b[36mh5py._proxy.dset_rw\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 28] Can't synchronously write data (file write failed: time = Wed Oct 15 04:08:26 2025\n, filename = 'model_weights_001_0.5025.weights.h5', file descriptor = 4, errno = 28, error message = 'No space left on device', buf = 0000017778E61040, total write size = 9437184, bytes this sub-write = 9437184, offset = 61289672)"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids, train_imagesX], \n",
    "    trainY_processed,\n",
    "    batch_size=256,  # Increased batch size for efficiency\n",
    "    epochs=20,       # Full training epochs\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    validation_data=(\n",
    "        [test_input_ids, test_input_masks, test_segment_ids, test_imagesX],\n",
    "        testY_processed\n",
    "    ),\n",
    "    callbacks=[early_stop]  # Only using early stopping (best weights kept in memory)\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training completed!\")\n",
    "print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f8fba54",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m fig, (ax1, ax2) = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Accuracy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m ax1.plot(\u001b[43mhistory\u001b[49m.history[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mTrain Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m ax1.plot(history.history[\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mVal Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m ax1.set_title(\u001b[33m'\u001b[39m\u001b[33mModel Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(history.history['loss'], label='Train Loss')\n",
    "ax2.plot(history.history['val_loss'], label='Val Loss')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4749746f",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model weights if needed (update filename with your best checkpoint)\n",
    "# Example: model.load_weights('model_weights_010_0.7769.weights.h5')\n",
    "\n",
    "# Or evaluate current model (after training)\n",
    "test_predictions = model.predict([\n",
    "    test_input_ids, test_input_masks, test_segment_ids, test_imagesX\n",
    "])\n",
    "test_predictions_binary = [1 if i >= 0.5 else 0 for i in test_predictions]\n",
    "\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(f\"Accuracy:  {accuracy_score(testY_processed, test_predictions_binary):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(testY_processed, test_predictions_binary, average=None)}\")\n",
    "print(f\"Precision: {precision_score(testY_processed, test_predictions_binary, average=None)}\")\n",
    "print(f\"Recall:    {recall_score(testY_processed, test_predictions_binary, average=None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8225a4",
   "metadata": {},
   "source": [
    "## 7. Inference on New Input\n",
    "\n",
    "Use this section to make predictions on new text + image pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fake_news(text, image_path, model, tokenizer, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict whether a news post (text + image) is fake or real.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The post text\n",
    "        image_path (str): Path to the image file\n",
    "        model: Trained Keras model\n",
    "        tokenizer: BERT tokenizer\n",
    "        threshold (float): Classification threshold (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    input_ids, input_masks, segment_ids = preprocess_text_input(\n",
    "        text, tokenizer, max_seq_length=max_seq_length\n",
    "    )\n",
    "    \n",
    "    # Preprocess image\n",
    "    image_data = preprocess_single_image(image_path, length=img_length, width=img_width)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(\n",
    "        [input_ids, input_masks, segment_ids, image_data],\n",
    "        verbose=0\n",
    "    )[0][0]\n",
    "    \n",
    "    # Classify\n",
    "    is_real = prediction >= threshold\n",
    "    label = \"REAL\" if is_real else \"FAKE\"\n",
    "    confidence = prediction if is_real else (1 - prediction)\n",
    "    \n",
    "    return {\n",
    "        'label': label,\n",
    "        'confidence': float(confidence),\n",
    "        'raw_score': float(prediction),\n",
    "        'text': text,\n",
    "        'image_path': image_path\n",
    "    }\n",
    "\n",
    "print(\"✓ Inference function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc687ab",
   "metadata": {},
   "source": [
    "### Example: Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Replace with your own text and image path\n",
    "\n",
    "sample_text = \"Breaking news: Major event happening now!\"\n",
    "sample_image_path = test_image_paths[0]  # Using first test image as example\n",
    "\n",
    "result = predict_fake_news(sample_text, sample_image_path, model, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTION RESULT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Text: {result['text']}\")\n",
    "print(f\"Image: {result['image_path']}\")\n",
    "print(f\"\\nPrediction: {result['label']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"Raw Score: {result['raw_score']:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Display the image\n",
    "img = cv2.imread(sample_image_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(f\"Prediction: {result['label']} ({result['confidence']:.2%} confidence)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0664c30",
   "metadata": {},
   "source": [
    "### Batch Prediction on Multiple Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(texts, image_paths, model, tokenizer, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make predictions on multiple text-image pairs.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings\n",
    "        image_paths (list): List of image file paths\n",
    "        model: Trained Keras model\n",
    "        tokenizer: BERT tokenizer\n",
    "        threshold (float): Classification threshold\n",
    "    \n",
    "    Returns:\n",
    "        list: List of prediction dictionaries\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for text, img_path in tqdm(zip(texts, image_paths), total=len(texts), desc=\"Making predictions\"):\n",
    "        try:\n",
    "            result = predict_fake_news(text, img_path, model, tokenizer, threshold)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            results.append({\n",
    "                'label': 'ERROR',\n",
    "                'confidence': 0.0,\n",
    "                'raw_score': 0.0,\n",
    "                'text': text,\n",
    "                'image_path': img_path,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    return results\n",
    "\n",
    "print(\"✓ Batch prediction function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict on first 5 test samples\n",
    "sample_size = 5\n",
    "sample_texts = test_text[:sample_size]\n",
    "sample_images = test_image_paths[:sample_size]\n",
    "sample_labels = testY[:sample_size]\n",
    "\n",
    "predictions = predict_batch(sample_texts, sample_images, model, tokenizer)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BATCH PREDICTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "for i, (pred, true_label) in enumerate(zip(predictions, sample_labels)):\n",
    "    true_label_str = \"REAL\" if true_label == 1 else \"FAKE\"\n",
    "    match = \"✓\" if pred['label'] == true_label_str else \"✗\"\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Text: {pred['text'][:50]}...\")\n",
    "    print(f\"  True Label: {true_label_str}\")\n",
    "    print(f\"  Predicted: {pred['label']} ({pred['confidence']:.2%}) {match}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623e65b",
   "metadata": {},
   "source": [
    "## 8. Save/Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c17fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights (more reliable than saving entire model)\n",
    "model.save_weights('spotfake_final_weights.weights.h5')\n",
    "print(\"✓ Model weights saved to 'spotfake_final_weights.weights.h5'\")\n",
    "\n",
    "# To load the weights later:\n",
    "# 1. First rebuild the model with same architecture\n",
    "# model = get_news_model(params_final)\n",
    "# 2. Then load the weights\n",
    "# model.load_weights('spotfake_final_weights.weights.h5')\n",
    "\n",
    "# Alternative: Save entire model (if needed for deployment)\n",
    "try:\n",
    "    model.save('spotfake_final_model.keras')  # Using new Keras format\n",
    "    print(\"✓ Full model saved to 'spotfake_final_model.keras'\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not save full model: {e}\")\n",
    "    print(\"Use save_weights() instead for more reliable saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d24e37",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. **Data Loading & Preprocessing**: Load Twitter fake news dataset with text and images\n",
    "2. **Model Architecture**: Multimodal model combining BERT (text) + VGG19 (image)\n",
    "3. **Training**: Train the model with optimal hyperparameters\n",
    "4. **Evaluation**: Comprehensive metrics on test set\n",
    "5. **Inference**: Easy-to-use functions for predicting on new inputs\n",
    "\n",
    "### To use this notebook:\n",
    "\n",
    "1. Run all cells in order to train the model\n",
    "2. Use `predict_fake_news()` for single predictions\n",
    "3. Use `predict_batch()` for multiple predictions\n",
    "4. Load saved weights with `model.load_weights()` to skip training\n",
    "\n",
    "### For new predictions:\n",
    "\n",
    "```python\n",
    "# Your custom input\n",
    "my_text = \"Your news text here\"\n",
    "my_image = \"path/to/your/image.jpg\"\n",
    "\n",
    "# Get prediction\n",
    "result = predict_fake_news(my_text, my_image, model, tokenizer)\n",
    "print(f\"Prediction: {result['label']} (Confidence: {result['confidence']:.2%})\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
